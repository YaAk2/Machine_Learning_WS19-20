{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the results to PDF\n",
    "\n",
    "Once you complete the assignments, export the entire notebook as PDF and attach it to your homework solutions. \n",
    "The best way of doing that is\n",
    "1. Run all the cells of the notebook.\n",
    "2. Export/download the notebook as PDF (File -> Download as -> PDF via LaTeX (.pdf)).\n",
    "3. Concatenate your solutions for other tasks with the output of Step 2. On linux, you can use `pdfunite`, there are similar tools for other platforms, too. You can only upload a single PDF file to Moodle.\n",
    "\n",
    "Make sure you are using `nbconvert` version 5.5 or later by running `jupyter nbconvert --version`. Older versions clip lines that exceed page width, which makes your code harder to grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def compare_images(img, img_compressed, k):\n",
    "    \"\"\"Show the compressed and uncompressed image side by side.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 12))\n",
    "    axes[0].set_axis_off()\n",
    "    if isinstance(k, str):\n",
    "        axes[0].set_title(k)\n",
    "    else:\n",
    "        axes[0].set_title(f\"Compressed to {k} colors\")\n",
    "    axes[0].imshow(img_compressed)\n",
    "    axes[1].set_axis_off()\n",
    "    axes[1].set_title(\"Original\")\n",
    "    axes[1].imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "In this first section you will implement the image compression algorithm from Bishop, chapter 9.1.1. Take an RGB image $X \\in \\mathbb{R}^{h \\times w \\times 3}$ and interpret it as a data matrix $X \\in \\mathbb{R}^{N \\times 3}$. Now apply $k$-means clustering to find $k$ colors that describe the image well and replace each pixel with its associated cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively try china.jpg\n",
    "X = load_sample_image(\"flower.jpg\")\n",
    "\n",
    "# or load your own image\n",
    "# X = np.array(Image.open(\"/home/user/path/to/some.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k):\n",
    "    \"\"\"Compute a k-means clustering for the data X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array of size N x D\n",
    "        where N is the number of samples and D is the data dimensionality\n",
    "    k : int\n",
    "        Number of clusters\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mu : np.array of size k x D\n",
    "        Cluster centers\n",
    "    z : np.array of size N\n",
    "        Cluster indicators, i.e. a number in 0..k - 1, for each data point in X\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Compute mu and z\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # Pick k random points to initialize mu\n",
    "    mu = X[np.random.choice(N, size=k, replace=False)].astype(np.float)\n",
    "    \n",
    "    z = np.empty(N, dtype=np.int)\n",
    "    J_prev = None\n",
    "    while True:\n",
    "        # Update the cluster indicators\n",
    "        z = ((X[:, np.newaxis] - mu[np.newaxis])**2).sum(axis=-1).argmin(axis=-1)\n",
    "        \n",
    "        # Count data points per cluster\n",
    "        N_k = np.bincount(z, minlength=k)\n",
    "        \n",
    "        # Restart if we lost any clusters\n",
    "        if np.any(N_k == 0):\n",
    "            mu = X[np.random.choice(N, size=k, replace=False)]\n",
    "            J_prev = None\n",
    "            continue\n",
    "            \n",
    "        # Update centroids\n",
    "        for i in range(k):\n",
    "            mu[i] = X[z == i].sum(axis=0)\n",
    "        mu = mu / N_k[:, np.newaxis]\n",
    "        \n",
    "        # Recompute the distortion\n",
    "        J = ((X - mu[z])**2).sum()\n",
    "        \n",
    "        # Check for convergence\n",
    "        if J_prev is None or J < J_prev:\n",
    "            J_prev = J\n",
    "        else:\n",
    "            break\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return mu, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the color values\n",
    "k = 5\n",
    "mu, z = kmeans(X.reshape((-1, 3)), k)\n",
    "\n",
    "# Replace each pixel with its cluster color\n",
    "X_compressed = mu[z].reshape(X.shape).astype(np.uint8)\n",
    "\n",
    "# Show the images side by side\n",
    "compare_images(X, X_compressed, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models & EM\n",
    "\n",
    "Now you will repeat the same exercise with GMMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_log_probability(X, pi, mu, sigma):\n",
    "    \"\"\"Compute the joint log-probabilities for each data point and component.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array of size N x D\n",
    "        where N is the number of samples and D is the data dimensionality\n",
    "    pi : np.array of size k\n",
    "        Prior weight of each component\n",
    "    mu : np.array of size k x D\n",
    "        Mean vectors of the k Gaussian component distributions\n",
    "    sigma : np.array of size k x D x D\n",
    "        Covariance matrices of the k Gaussian component distributions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    P : np.array of shape N x k\n",
    "        P[i, j] is the joint log-probability of data point i under component j\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Compute P\n",
    "    \n",
    "    ## BEGIN SOLUTION\n",
    "    # Precompute the log weights\n",
    "    log_pi = np.log(pi)\n",
    "    \n",
    "    # The constant factor in the normal distribution PDF\n",
    "    k = pi.shape[0]\n",
    "    consts = -k / 2*np.log(2*np.pi)\n",
    "    \n",
    "    # Precompute the matrix log determinants and inverses\n",
    "    log_det = np.log(np.linalg.det(sigma))\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    \n",
    "    # Find the pairwise distance between any data point and cluster center\n",
    "    d = X[:, np.newaxis] - mu[np.newaxis]\n",
    "    \n",
    "    P = log_pi + consts - log_det / 2 - (d[..., np.newaxis, :] @ sigma_inv @ d[..., np.newaxis]).squeeze() / 2\n",
    "    ## END SOLUTION\n",
    "    \n",
    "    return P\n",
    "    \n",
    "\n",
    "def em(X, k, tol=0.001):\n",
    "    \"\"\"Fit a Gaussian mixture model with k components to X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array of size N x D\n",
    "        where N is the number of samples and D is the data dimensionality\n",
    "    k : int\n",
    "        Number of clusters\n",
    "    tol : float\n",
    "        Converge when the increase in the mean of the expected joint log-likelihood\n",
    "        is lower than this\n",
    "        \n",
    "    The algorithm should stop when the relative improvement in the optimization\n",
    "    objective is less than rtol.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pi : np.array of size k\n",
    "        Prior weight of each component\n",
    "    mu : np.array of size k x D\n",
    "        Mean vectors of the k Gaussian component distributions\n",
    "    sigma : np.array of size k x D x D\n",
    "        Covariance matrices of the k Gaussian component distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Compute pi, mu, sigma\n",
    "    \n",
    "    ## BEGIN SOLUTION\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    pi = np.full(k, 1 / k)\n",
    "    mu = X[np.random.choice(N, size=k, replace=False)].astype(np.float)\n",
    "    sigma = np.identity(D)[np.newaxis].repeat(k, axis=0)\n",
    "    \n",
    "    log_probability = gmm_log_probability(X, pi, mu, sigma)\n",
    "    J_prev = None\n",
    "    einpath = None\n",
    "    while True:\n",
    "        # E step\n",
    "        # Exp-normalize trick\n",
    "        z = np.exp(log_probability - log_probability.max(axis=1, keepdims=True))\n",
    "        z = z / z.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # M step\n",
    "        N_k = z.sum(axis=0)\n",
    "        pi = N_k / N\n",
    "        mu = z.T @ X / N_k[:, np.newaxis]\n",
    "        d = X[:, np.newaxis] - mu[np.newaxis]\n",
    "        \n",
    "        # Cache the optimal way to compute sigma\n",
    "        if einpath is None:\n",
    "            einpath = np.einsum_path(\"nkd,nkD,nk,k->kdD\", d, d, z, 1 / N_k, optimize=\"optimal\")[0]\n",
    "        sigma = np.einsum(\"nkd,nkD,nk,k->kdD\", d, d, z, 1 / N_k, optimize=einpath)\n",
    "        \n",
    "        # Compute the mean expected joint log-probability\n",
    "        log_probability = gmm_log_probability(X, pi, mu, sigma)\n",
    "        J = (log_probability * z).sum(axis=1).mean()\n",
    "        \n",
    "        # Check for convergence\n",
    "        if J_prev is None or J - J_prev > tol:\n",
    "            J_prev = J\n",
    "        else:\n",
    "            break\n",
    "    ## END SOLUTION\n",
    "    \n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit the GMM\n",
    "k = 5\n",
    "pi, mu, sigma = em(X.reshape((-1, 3)), k)\n",
    "\n",
    "# Determine the most likely cluster of each pixel\n",
    "log_p = gmm_log_probability(X.reshape((-1, 3)), pi, mu, sigma)\n",
    "z = log_p.argmax(axis=1)\n",
    "\n",
    "# Replace each pixel with its cluster mean\n",
    "X_compressed = mu[z].reshape(X.shape).astype(np.uint8)\n",
    "\n",
    "# Show the images side by side\n",
    "compare_images(X, X_compressed, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Unseen Datapoints\n",
    "\n",
    "You have trained a generative model which allows you to sample from the learned distribution. In this section, you sample new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_sample(N, pi, mu, sigma):\n",
    "    \"\"\"Sample N data points from a Gaussian mixture model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of data points to sample\n",
    "    pi : np.array of size k\n",
    "        Prior weight of each component\n",
    "    mu : np.array of size k x D\n",
    "        Mean vectors of the k Gaussian component distributions\n",
    "    sigma : np.array of size k x D x D\n",
    "        Covariance matrices of the k Gaussian component distributions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.array of shape N x D\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Sample X\n",
    "    \n",
    "    ## BEGIN SOLUTION\n",
    "    k, D = mu.shape\n",
    "    z = np.random.choice(k, size=N, replace=True, p=pi)\n",
    "    X = np.empty((N, D))\n",
    "    for i in range(k):\n",
    "        filter_i = z == i\n",
    "        n_i = np.count_nonzero(filter_i)\n",
    "        X[filter_i] = np.random.multivariate_normal(mu[i], sigma[i], size=n_i)\n",
    "    ## END SOLUTION\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample pixels and reshape them into the size of the original image\n",
    "X_sampled = gmm_sample(np.prod(X.shape[:-1]), pi, mu, sigma).reshape(X.shape).astype(np.uint8)\n",
    "\n",
    "# Compare the original and the sampled image\n",
    "compare_images(X, X_sampled, \"Sampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what you see in the generated images. (1-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEGIN SOLUTION\n",
    "A GMM assumes that the data points are independent and identically distributed. Therefore, a pixel grid sampled from a GMM will look like noise (with a comparable color distribution to the training image) because in natural images values between adjacent pixel are strongly correlated.\n",
    "## END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
